{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb673a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting the TDS Course Content DEEP scraping process...\n",
      "Current UTC time: 2025-06-11T02:57:04.904119+00:00\n",
      "==================================================\n",
      "--- [Course Scraper] Starting Deep Scrape ---\n",
      "[Course Scraper] WebDriver initialized.\n",
      "[Course Scraper] Navigating to home page: https://tds.s-anand.net/#/2025-01/\n",
      "[Course Scraper] Home page loaded.\n",
      "[Course Scraper] Found 8 main modules to process.\n",
      "\n",
      "[Course Scraper] (1/8) Processing MAIN MODULE: 'Development Tools'\n",
      "  -> Scraping landing page for 'Development Tools'...\n",
      "  -> Found and processed 3 text chunks.\n",
      "\n",
      "[Course Scraper] (2/8) Processing MAIN MODULE: 'Deployment Tools'\n",
      "  -> Scraping landing page for 'Deployment Tools'...\n",
      "  -> Found and processed 1 text chunks.\n",
      "\n",
      "[Course Scraper] (3/8) Processing MAIN MODULE: 'Large Language Models'\n",
      "  -> Scraping landing page for 'Large Language Models'...\n",
      "  -> Found and processed 14 text chunks.\n",
      "\n",
      "[Course Scraper] (4/8) Processing MAIN MODULE: 'Data Sourcing'\n",
      "  -> Scraping landing page for 'Data Sourcing'...\n",
      "  -> Found and processed 14 text chunks.\n",
      "\n",
      "[Course Scraper] (5/8) Processing MAIN MODULE: 'Data Preparation'\n",
      "  -> Scraping landing page for 'Data Preparation'...\n",
      "  -> Found and processed 11 text chunks.\n",
      "\n",
      "[Course Scraper] (6/8) Processing MAIN MODULE: 'Data Analysis'\n",
      "  -> Scraping landing page for 'Data Analysis'...\n",
      "  -> Found and processed 9 text chunks.\n",
      "\n",
      "[Course Scraper] (7/8) Processing MAIN MODULE: 'Data Visualization'\n",
      "  -> Scraping landing page for 'Data Visualization'...\n",
      "  -> Found and processed 0 text chunks.\n",
      "\n",
      "[Course Scraper] (8/8) Processing MAIN MODULE: 'Live Sessions'\n",
      "  -> Scraping landing page for 'Live Sessions'...\n",
      "  -> Found and processed 5 text chunks.\n",
      "\n",
      "--- [Course Scraper] Finished Deep Scrape. Found a total of 57 chunks. ---\n",
      "\n",
      "✅ Success! Deep scrape data saved to 'tds_course_content_deep.json'. Total chunks: 57\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone  # This import is important\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Configuration ---\n",
    "COURSE_WEBSITE_URL = \"https://tds.s-anand.net/#/2025-01/\"\n",
    "OUTPUT_FILENAME = \"tds_course_content_deep.json\"\n",
    "\n",
    "# REMOVED: The helper function is no longer needed to prevent the NameError.\n",
    "# def get_utc_now_iso():\n",
    "#     \"\"\"Returns the current UTC time in ISO 8601 format.\"\"\"\n",
    "#     return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def scrape_page_content(soup, url, module_name):\n",
    "    \"\"\"\n",
    "    A helper function to scrape chunks from a single loaded page.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    article = soup.find('article', id='main')\n",
    "    if not article:\n",
    "        print(f\"  -> ERROR: Could not find the <article id='main'> tag on page {url}. Skipping.\")\n",
    "        return []\n",
    "\n",
    "    page_title = article.find('h1').get_text(strip=True) if article.find('h1') else \"Untitled\"\n",
    "    page_chunks = 0\n",
    "    \n",
    "    for element in article.find_all(['p', 'li', 'tr', 'h2', 'h3']):\n",
    "        content_text = element.get_text(strip=True)\n",
    "        if len(content_text) > 10:\n",
    "            page_chunks += 1\n",
    "            documents.append({\n",
    "                \"source_url\": url,\n",
    "                \"source_type\": \"course_content\",\n",
    "                \"title\": page_title,\n",
    "                \"content\": content_text,\n",
    "                \"metadata\": {\n",
    "                    # MODIFIED: Placed the datetime logic directly here.\n",
    "                    \"scraped_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"module\": module_name\n",
    "                }\n",
    "            })\n",
    "    print(f\"  -> Found and processed {page_chunks} text chunks.\")\n",
    "    return documents\n",
    "\n",
    "def scrape_course_content():\n",
    "    \"\"\"\n",
    "    Performs a deep scrape of the course website, including all nested submodule pages.\n",
    "    \"\"\"\n",
    "    print(\"--- [Course Scraper] Starting Deep Scrape ---\")\n",
    "    \n",
    "    all_documents = []\n",
    "    driver = None\n",
    "    try:\n",
    "        service = Service(EdgeChromiumDriverManager().install())\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument(\"window-size=1920,1080\")\n",
    "        options.add_argument(\"--log-level=3\")\n",
    "        driver = webdriver.Edge(service=service, options=options)\n",
    "        print(\"[Course Scraper] WebDriver initialized.\")\n",
    "\n",
    "        print(f\"[Course Scraper] Navigating to home page: {COURSE_WEBSITE_URL}\")\n",
    "        driver.get(COURSE_WEBSITE_URL)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"aside.sidebar\")))\n",
    "        print(\"[Course Scraper] Home page loaded.\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        main_module_links = []\n",
    "        for link in soup.select('aside.sidebar .sidebar-nav > ul > li.folder > a'):\n",
    "            if link.has_attr('title') and link.has_attr('href'):\n",
    "                main_module_links.append({\n",
    "                    'name': link['title'],\n",
    "                    'href': link['href']\n",
    "                })\n",
    "        \n",
    "        if not main_module_links:\n",
    "            print(\"[Course Scraper] CRITICAL: No main module links found. Exiting.\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"[Course Scraper] Found {len(main_module_links)} main modules to process.\")\n",
    "\n",
    "        for i, module in enumerate(main_module_links):\n",
    "            module_name = module['name']\n",
    "            module_url = \"https://tds.s-anand.net/\" + module['href']\n",
    "            print(f\"\\n[Course Scraper] ({i+1}/{len(main_module_links)}) Processing MAIN MODULE: '{module_name}'\")\n",
    "            \n",
    "            driver.get(module_url)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"article#main h1\")))\n",
    "            time.sleep(1)\n",
    "            \n",
    "            print(f\"  -> Scraping landing page for '{module_name}'...\")\n",
    "            current_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            all_documents.extend(scrape_page_content(current_soup, module_url, module_name))\n",
    "\n",
    "            submodule_links = []\n",
    "            for sub_link in current_soup.select('aside.sidebar li.folder.active > ul > li > a'):\n",
    "                 if sub_link.has_attr('title') and sub_link.has_attr('href'):\n",
    "                    submodule_links.append({\n",
    "                        'name': sub_link['title'],\n",
    "                        'href': sub_link['href']\n",
    "                    })\n",
    "\n",
    "            if submodule_links:\n",
    "                print(f\"  -> Found {len(submodule_links)} submodules. Scraping them now...\")\n",
    "                for j, submodule in enumerate(submodule_links):\n",
    "                    submodule_name = submodule['name']\n",
    "                    submodule_url = \"https://tds.s-anand.net/\" + submodule['href']\n",
    "                    print(f\"    - ({j+1}/{len(submodule_links)}) Scraping SUBMODULE: '{submodule_name}'\")\n",
    "                    \n",
    "                    driver.get(submodule_url)\n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"article#main h1\")))\n",
    "                    \n",
    "                    submodule_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    all_documents.extend(scrape_page_content(submodule_soup, submodule_url, module_name))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[Course Scraper] An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "        \n",
    "    print(f\"\\n--- [Course Scraper] Finished Deep Scrape. Found a total of {len(all_documents)} chunks. ---\")\n",
    "    return all_documents\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*50)\n",
    "    print(\"Starting the TDS Course Content DEEP scraping process...\")\n",
    "    # MODIFIED: Placed the datetime logic directly here.\n",
    "    print(f\"Current UTC time: {datetime.now(timezone.utc).isoformat()}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    course_documents = scrape_course_content()\n",
    "    \n",
    "    if not course_documents:\n",
    "        print(\"\\nCRITICAL WARNING: The final dataset is empty. Please check the logs.\")\n",
    "    else:\n",
    "        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "            json.dump(course_documents, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\n✅ Success! Deep scrape data saved to '{OUTPUT_FILENAME}'. Total chunks: {len(course_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f15a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
