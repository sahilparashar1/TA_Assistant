{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10de569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting IIT Madras TDS Course Content Scraping (HTML Only) ---\n",
      "WARNING: This script is a template. You MUST customize `BASE_URL`.\n",
      "Also, you might need to adjust the CSS selectors/HTML parsing logic \n",
      "if the actual page structure differs from the screenshot.\n",
      "Ensure you have permission to scrape the website.\n",
      "Starting course content scraping from https://tds.s-anand.net/#/2025-01/\n",
      "Fetching: https://tds.s-anand.net/#/2025-01/\n",
      "Warning: Could not find 'Tools in Data Science - Jan 2025' title. Scraping all text from main page.\n",
      "Saved: scraped_tds_course_content\\course_overview_fallback.txt\n",
      "Found 0 unique module/project content links.\n",
      "\n",
      "Course content scraping finished.\n",
      "--- Scraping script execution complete ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress the UserWarning from BeautifulSoup if no parser is explicitly specified\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# --- Configuration ---\n",
    "# BASE_URL: The main URL shown in the screenshot for \"Tools in Data Science - Jan 2025\"\n",
    "# Replace with the actual URL of your IIT Madras TDS course content page.\n",
    "# For example: \"https://online.iitm.ac.in/courses/tds/jan2025\" (This is a placeholder)\n",
    "BASE_URL = \"https://tds.s-anand.net/#/2025-01/\"\n",
    "\n",
    "# Output directory to save scraped course content\n",
    "COURSE_CONTENT_OUTPUT_DIR = \"scraped_tds_course_content\"\n",
    "\n",
    "# Delay between requests (to be polite and avoid being blocked)\n",
    "REQUEST_DELAY = 1.5 # seconds\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"Fetches the content of a given URL.\"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, timeout=15) # Increased timeout\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        time.sleep(REQUEST_DELAY) # Be polite\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_content(filename, content, directory=COURSE_CONTENT_OUTPUT_DIR):\n",
    "    \"\"\"Saves content to a file in the specified directory.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving {filepath}: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_html_text(html_content):\n",
    "    \"\"\"Removes common HTML tags and cleans up text, focusing on main content.\"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove script, style, and common navigation/layout elements\n",
    "    for element in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'img', 'svg']):\n",
    "        element.decompose()\n",
    "\n",
    "    text = soup.get_text(separator='\\n')\n",
    "\n",
    "    # Remove excessive whitespace, newlines, and tabs\n",
    "    text = re.sub(r'\\n+', '\\n', text) # Replace multiple newlines with single\n",
    "    text = re.sub(r' +', ' ', text)   # Replace multiple spaces with single\n",
    "    text = text.strip()               # Remove leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "def normalize_filename(text):\n",
    "    \"\"\"Cleans text to be suitable for a filename.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\-_\\. ]', '', text) # Remove invalid characters\n",
    "    text = re.sub(r'[ ]+', '_', text)    # Replace spaces with underscores\n",
    "    text = text.strip('_')\n",
    "    return text[:100] # Limit filename length to avoid OS issues\n",
    "\n",
    "# --- Main Scraping Logic ---\n",
    "\n",
    "def scrape_tds_course_content():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping of course content based on the provided screenshot,\n",
    "    ignoring discussion threads.\n",
    "    \"\"\"\n",
    "    print(f\"Starting course content scraping from {BASE_URL}\")\n",
    "    index_page_html = get_page_content(BASE_URL)\n",
    "\n",
    "    if not index_page_html:\n",
    "        print(\"Could not retrieve the main course page. Exiting.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(index_page_html, 'html.parser')\n",
    "\n",
    "    # --- 1. Scrape the main overview text ---\n",
    "    title_element = soup.find('h2', string='Tools in Data Science - Jan 2025')\n",
    "    \n",
    "    overview_text_content = \"\"\n",
    "    if title_element:\n",
    "        current_element = title_element\n",
    "        # Iterate through siblings until we hit a clear end to the overview (e.g., the module list)\n",
    "        while current_element and not (current_element.name == 'ul' or current_element.find('li')):\n",
    "            # Stop if we encounter a div that clearly marks the start of module list\n",
    "            # (e.g., if there's a div wrapping the module list, inspect its class/id)\n",
    "            if current_element.name == 'div' and ('main-content' in current_element.get('class', []) or 'course-sections' in current_element.get('class', [])):\n",
    "                break # Break if we hit a container for the modules\n",
    "            \n",
    "            # Append text from p, div, etc.\n",
    "            if current_element.name in ['p', 'div', 'h3', 'span']: # Add other relevant tags\n",
    "                 overview_text_content += current_element.get_text(separator='\\n').strip() + '\\n'\n",
    "            \n",
    "            # Special handling for elements like <ul> that might contain list items\n",
    "            # (e.g., \"This course exposes you to real-life tools\" items)\n",
    "            if current_element.name == 'ul':\n",
    "                for li in current_element.find_all('li'):\n",
    "                    overview_text_content += \"- \" + li.get_text().strip() + '\\n'\n",
    "\n",
    "            current_element = current_element.find_next_sibling()\n",
    "        \n",
    "        # Clean up the collected overview text\n",
    "        overview_text_content = re.sub(r'\\n+', '\\n', overview_text_content).strip()\n",
    "        save_content(\"course_overview.txt\", overview_text_content)\n",
    "    else:\n",
    "        print(\"Warning: Could not find 'Tools in Data Science - Jan 2025' title. Scraping all text from main page.\")\n",
    "        # Fallback: Scrape all clean text from the main page if specific title not found\n",
    "        cleaned_main_page_text = clean_html_text(index_page_html)\n",
    "        if cleaned_main_page_text:\n",
    "            save_content(\"course_overview_fallback.txt\", cleaned_main_page_text)\n",
    "\n",
    "\n",
    "    # --- 2. Identify Module and Project Links ---\n",
    "    module_links_found = []\n",
    "    \n",
    "    # Target the main content block that holds module links based on the screenshot\n",
    "    # The links \"1. Development Tools\", \"2. Deployment Tools\", etc., are likely within a specific div.\n",
    "    # We'll try to find any <a> tag whose text starts with a number and a dot,\n",
    "    # or contains \"Project\" followed by a number.\n",
    "\n",
    "    # A more robust way would be to find the direct parent of these numbered lists/links.\n",
    "    # E.g., if they are in a div with id=\"course-modules-list\"\n",
    "    # main_modules_container = soup.find('div', id='course-modules-list')\n",
    "    \n",
    "    # If no specific container, we can search the entire body for links matching the pattern.\n",
    "    # The screenshot shows the module links are regular text followed by a link, not nested deeply.\n",
    "    \n",
    "    # Look for links that represent the actual module/project pages\n",
    "    # We'll refine this by looking for specific text patterns\n",
    "    all_potential_links_on_page = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_potential_links_on_page:\n",
    "        href = link.get('href')\n",
    "        text = link.get_text().strip()\n",
    "        \n",
    "        if not href or href.startswith(('#', 'javascript:')):\n",
    "            continue\n",
    "        \n",
    "        # Exclude \"Discussion Thread\" links explicitly\n",
    "        if \"discussion thread\" in text.lower():\n",
    "            continue\n",
    "            \n",
    "        # Construct absolute URL\n",
    "        if not href.startswith(('http://', 'https://')):\n",
    "            abs_url = requests.compat.urljoin(BASE_URL, href)\n",
    "        else:\n",
    "            abs_url = href\n",
    "        \n",
    "        # Filter for module/project links: starts with \"Number.\" or contains \"Project Number\"\n",
    "        # Example: \"1. Development Tools\", \"Project 1\"\n",
    "        if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE):\n",
    "            # Check if the URL is relevant (e.g., stays within the course domain)\n",
    "            if abs_url.startswith(BASE_URL.split('/')[0] + '//' + BASE_URL.split('/')[2]):\n",
    "                module_links_found.append({'url': abs_url, 'text': text})\n",
    "            else:\n",
    "                print(f\"Skipping external link for module: {text} -> {abs_url}\")\n",
    "        \n",
    "    # Remove duplicates if any link is found multiple times\n",
    "    unique_module_links = []\n",
    "    processed_urls_for_modules = set()\n",
    "    for link_info in module_links_found:\n",
    "        if link_info['url'] not in processed_urls_for_modules:\n",
    "            unique_module_links.append(link_info)\n",
    "            processed_urls_for_modules.add(link_info['url'])\n",
    "\n",
    "    print(f\"Found {len(unique_module_links)} unique module/project content links.\")\n",
    "\n",
    "    # --- 3. Visit Each Module Page and Scrape Content ---\n",
    "    for i, link_info in enumerate(unique_module_links):\n",
    "        url = link_info['url']\n",
    "        original_link_text = link_info['text']\n",
    "        filename_prefix = normalize_filename(original_link_text)\n",
    "        \n",
    "        print(f\"\\nProcessing content from module: {original_link_text} ({url})\")\n",
    "\n",
    "        module_page_html = get_page_content(url)\n",
    "        if module_page_html:\n",
    "            cleaned_text = clean_html_text(module_page_html)\n",
    "            if cleaned_text:\n",
    "                filename = f\"{filename_prefix}.txt\"\n",
    "                save_content(filename, cleaned_text)\n",
    "            else:\n",
    "                print(f\"Warning: No significant text extracted from {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for module: {original_link_text} ({url})\")\n",
    "\n",
    "    print(\"\\nCourse content scraping finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(COURSE_CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"--- Starting IIT Madras TDS Course Content Scraping (HTML Only) ---\")\n",
    "    print(\"WARNING: This script is a template. You MUST customize `BASE_URL`.\")\n",
    "    print(\"Also, you might need to adjust the CSS selectors/HTML parsing logic \")\n",
    "    print(\"if the actual page structure differs from the screenshot.\")\n",
    "    print(\"Ensure you have permission to scrape the website.\")\n",
    "    \n",
    "    # You might need to add authentication logic here if the course content is behind a login.\n",
    "    # (e.g., using requests.Session() and handling a login form POST request)\n",
    "    \n",
    "    scrape_tds_course_content()\n",
    "    print(\"--- Scraping script execution complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88880b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting IIT Madras TDS Course Content Scraping (HTML Only) ---\n",
      "WARNING: This script is a template. You MUST customize `BASE_URL`.\n",
      "Also, you might need to adjust the CSS selectors/HTML parsing logic \n",
      "if the actual page structure differs from the screenshot.\n",
      "Ensure you have permission to scrape the website.\n",
      "Starting course content scraping from https://tds.s-anand.net/#/2025-01/\n",
      "Fetching: https://tds.s-anand.net/#/2025-01/\n",
      "Saved: scraped_tds_course_content\\course_overview.txt\n",
      "Extracted overview text.\n",
      "Found 0 unique module/project content links.\n",
      "\n",
      "Course content scraping finished.\n",
      "--- Scraping script execution complete ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress the UserWarning from BeautifulSoup if no parser is explicitly specified\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# --- Configuration ---\n",
    "# BASE_URL: The main URL shown in the screenshot for \"Tools in Data Science - Jan 2025\"\n",
    "# Replace with the actual URL of your IIT Madras TDS course content page.\n",
    "# For example: \"https://online.iitm.ac.in/courses/tds/jan2025\" (This is a placeholder)\n",
    "BASE_URL = \"https://tds.s-anand.net/#/2025-01/\"\n",
    "\n",
    "# Output directory to save scraped course content\n",
    "COURSE_CONTENT_OUTPUT_DIR = \"scraped_tds_course_content\"\n",
    "\n",
    "# Delay between requests (to be polite and avoid being blocked)\n",
    "REQUEST_DELAY = 1.5 # seconds\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"Fetches the content of a given URL.\"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, timeout=15) # Increased timeout\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        time.sleep(REQUEST_DELAY) # Be polite\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_content(filename, content, directory=COURSE_CONTENT_OUTPUT_DIR):\n",
    "    \"\"\"Saves content to a file in the specified directory.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving {filepath}: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_html_text(html_element):\n",
    "    \"\"\"\n",
    "    Removes common HTML tags and cleans up text from a given BeautifulSoup element,\n",
    "    focusing on main content.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original soup object if needed elsewhere\n",
    "    temp_soup = BeautifulSoup(str(html_element), 'html.parser')\n",
    "\n",
    "    # Remove script, style, and common navigation/layout elements\n",
    "    for element in temp_soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'img', 'svg', 'a']): # Exclude 'a' for overview only\n",
    "        element.decompose()\n",
    "\n",
    "    text = temp_soup.get_text(separator='\\n')\n",
    "\n",
    "    # Remove excessive whitespace, newlines, and tabs\n",
    "    text = re.sub(r'\\n+', '\\n', text) # Replace multiple newlines with single\n",
    "    text = re.sub(r' +', ' ', text)   # Replace multiple spaces with single\n",
    "    text = text.strip()               # Remove leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "def normalize_filename(text):\n",
    "    \"\"\"Cleans text to be suitable for a filename.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\-_\\. ]', '', text) # Remove invalid characters\n",
    "    text = re.sub(r'[ ]+', '_', text)    # Replace spaces with underscores\n",
    "    text = text.strip('_')\n",
    "    return text[:100] # Limit filename length to avoid OS issues\n",
    "\n",
    "# --- Main Scraping Logic ---\n",
    "\n",
    "def scrape_tds_course_content():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping of course content based on the provided screenshot,\n",
    "    ignoring discussion threads.\n",
    "    \"\"\"\n",
    "    print(f\"Starting course content scraping from {BASE_URL}\")\n",
    "    index_page_html = get_page_content(BASE_URL)\n",
    "\n",
    "    if not index_page_html:\n",
    "        print(\"Could not retrieve the main course page. Exiting.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(index_page_html, 'html.parser')\n",
    "\n",
    "    # --- 1. Scrape the main overview text ---\n",
    "    # Attempt to find a main content area, or default to body.\n",
    "    # The screenshot suggests the overview text is in a main content block, possibly not deeply nested.\n",
    "    \n",
    "    # Try to find a div or section that seems to contain the main content.\n",
    "    # This is a common pattern: <main>, <div id=\"content\">, <div class=\"main-body\">, etc.\n",
    "    # You might need to inspect the HTML to find the correct container.\n",
    "    main_content_container = soup.find('main') or soup.find('div', class_='main-content') or soup.find('div', id='content') or soup.find('body')\n",
    "    \n",
    "    if main_content_container:\n",
    "        # Clone the container's soup to remove links without affecting link discovery later\n",
    "        overview_soup = BeautifulSoup(str(main_content_container), 'html.parser')\n",
    "        \n",
    "        # Remove elements that are part of the module list structure\n",
    "        # This is a heuristic. You might need to refine this by looking for specific module list containers.\n",
    "        for module_link_container in overview_soup.find_all('a', href=True):\n",
    "            text = module_link_container.get_text().strip()\n",
    "            if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE) or \"discussion thread\" in text.lower():\n",
    "                # If this link is part of the modules, remove its parent element or itself\n",
    "                if module_link_container.parent: # Try to remove the list item or paragraph containing it\n",
    "                    module_link_container.parent.decompose()\n",
    "                else: # Fallback: just remove the link itself\n",
    "                    module_link_container.decompose()\n",
    "        \n",
    "        # After removing module-related elements, get the remaining text as overview\n",
    "        overview_text_content = clean_html_text(overview_soup) # Use the modified clean_html_text function here\n",
    "        save_content(\"course_overview.txt\", overview_text_content)\n",
    "        print(\"Extracted overview text.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not find a suitable main content container. Overview text might be incomplete.\")\n",
    "        save_content(\"course_overview_fallback.txt\", clean_html_text(soup)) # Fallback: clean entire page for overview\n",
    "\n",
    "    # --- 2. Identify Module and Project Links ---\n",
    "    module_links_found = []\n",
    "    \n",
    "    # We now search for links on the *original* soup object before modification for overview\n",
    "    all_potential_links_on_page = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_potential_links_on_page:\n",
    "        href = link.get('href')\n",
    "        text = link.get_text().strip()\n",
    "        \n",
    "        if not href or href.startswith(('#', 'javascript:')):\n",
    "            continue\n",
    "        \n",
    "        # Exclude \"Discussion Thread\" links explicitly\n",
    "        if \"discussion thread\" in text.lower():\n",
    "            continue\n",
    "            \n",
    "        # Construct absolute URL\n",
    "        if not href.startswith(('http://', 'https://')):\n",
    "            abs_url = requests.compat.urljoin(BASE_URL, href)\n",
    "        else:\n",
    "            abs_url = href\n",
    "        \n",
    "        # Filter for module/project links: starts with \"Number.\" or contains \"Project Number\"\n",
    "        if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE):\n",
    "            # Check if the URL is relevant (e.g., stays within the course domain)\n",
    "            if abs_url.startswith(BASE_URL.split('/')[0] + '//' + BASE_URL.split('/')[2]):\n",
    "                module_links_found.append({'url': abs_url, 'text': text})\n",
    "            else:\n",
    "                print(f\"Skipping external link for module: {text} -> {abs_url}\")\n",
    "        \n",
    "    # Remove duplicates if any link is found multiple times\n",
    "    unique_module_links = []\n",
    "    processed_urls_for_modules = set()\n",
    "    for link_info in module_links_found:\n",
    "        if link_info['url'] not in processed_urls_for_modules:\n",
    "            unique_module_links.append(link_info)\n",
    "            processed_urls_for_modules.add(link_info['url'])\n",
    "\n",
    "    print(f\"Found {len(unique_module_links)} unique module/project content links.\")\n",
    "\n",
    "    # --- 3. Visit Each Module Page and Scrape Content ---\n",
    "    for i, link_info in enumerate(unique_module_links):\n",
    "        url = link_info['url']\n",
    "        original_link_text = link_info['text']\n",
    "        filename_prefix = normalize_filename(original_link_text)\n",
    "        \n",
    "        print(f\"\\nProcessing content from module: {original_link_text} ({url})\")\n",
    "\n",
    "        module_page_html = get_page_content(url)\n",
    "        if module_page_html:\n",
    "            cleaned_text = clean_html_text(BeautifulSoup(module_page_html, 'html.parser')) # Pass soup object to clean_html_text\n",
    "            if cleaned_text:\n",
    "                filename = f\"{filename_prefix}.txt\"\n",
    "                save_content(filename, cleaned_text)\n",
    "            else:\n",
    "                print(f\"Warning: No significant text extracted from {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for module: {original_link_text} ({url})\")\n",
    "\n",
    "    print(\"\\nCourse content scraping finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure output directories exist\n",
    "    os.makedirs(COURSE_CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"--- Starting IIT Madras TDS Course Content Scraping (HTML Only) ---\")\n",
    "    print(\"WARNING: This script is a template. You MUST customize `BASE_URL`.\")\n",
    "    print(\"Also, you might need to adjust the CSS selectors/HTML parsing logic \")\n",
    "    print(\"if the actual page structure differs from the screenshot.\")\n",
    "    print(\"Ensure you have permission to scrape the website.\")\n",
    "    \n",
    "    # You might need to add authentication logic here if the course content is behind a login.\n",
    "    # (e.g., using requests.Session() and handling a login form POST request)\n",
    "    \n",
    "    scrape_tds_course_content()\n",
    "    print(\"--- Scraping script execution complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c4ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting IIT Madras TDS Course Content Scraping (HTML Only) ---\n",
      "WARNING: This script is a template. You MUST customize `BASE_URL`.\n",
      "If output is blank, check the 'raw_*.html' files in the debug directory.\n",
      "Remember to handle authentication if the content is behind a login.\n",
      "Ensure you have permission to scrape the website.\n",
      "Starting course content scraping from https://tds.s-anand.net/#/2025-01/\n",
      "Attempting to fetch: https://tds.s-anand.net/#/2025-01/\n",
      "Successfully fetched https://tds.s-anand.net/#/2025-01/. Status Code: 200\n",
      "Saved raw HTML to: scraped_tds_course_content_debug\\raw__tds.s-anand.net__2025-01_.html\n",
      "Warning: Overview text extraction resulted in blank content. Adjusting selectors needed.\n",
      "Saved: scraped_tds_course_content_debug\\course_overview_blank_debug.txt\n",
      "Found 0 unique module/project content links.\n",
      "\n",
      "Course content scraping finished.\n",
      "--- Scraping script execution complete ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_URL = \"https://tds.s-anand.net/#/2025-01/\" # <--- CONFIRM THIS!\n",
    "COURSE_CONTENT_OUTPUT_DIR = \"scraped_tds_course_content_debug\" # Use a new debug directory\n",
    "REQUEST_DELAY = 2 # seconds - increased slightly\n",
    "\n",
    "# Add a User-Agent header to mimic a browser\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"Fetches the content of a given URL.\"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to fetch: {url}\")\n",
    "        response = requests.get(url, timeout=15, headers=HEADERS) # Pass headers\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        print(f\"Successfully fetched {url}. Status Code: {response.status_code}\")\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        # --- DEBUGGING STEP: Save raw HTML ---\n",
    "        raw_html_filename = normalize_filename(f\"raw__{url.replace('https://', '').replace('/', '_')[:50]}.html\")\n",
    "        raw_html_filepath = os.path.join(COURSE_CONTENT_OUTPUT_DIR, raw_html_filename)\n",
    "        with open(raw_html_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"Saved raw HTML to: {raw_html_filepath}\")\n",
    "        # --- END DEBUGGING STEP ---\n",
    "        \n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error fetching {url}: {e.response.status_code} - {e.response.reason}\")\n",
    "        print(f\"Response body (if available): {e.response.text[:500]}...\") # Print first 500 chars\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"General Request Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# (Keep save_content, clean_html_text, normalize_filename functions as they were in the last script)\n",
    "# ... [copy paste from previous script starting here] ...\n",
    "def save_content(filename, content, directory=COURSE_CONTENT_OUTPUT_DIR):\n",
    "    \"\"\"Saves content to a file in the specified directory.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving {filepath}: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_html_text(html_element):\n",
    "    \"\"\"\n",
    "    Removes common HTML tags and cleans up text from a given BeautifulSoup element,\n",
    "    focusing on main content.\n",
    "    \"\"\"\n",
    "    temp_soup = BeautifulSoup(str(html_element), 'html.parser')\n",
    "\n",
    "    # Remove script, style, and common navigation/layout elements\n",
    "    for element in temp_soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'img', 'svg', 'a']): # Keep 'a' for module pages, remove for overview\n",
    "        element.decompose()\n",
    "\n",
    "    text = temp_soup.get_text(separator='\\n')\n",
    "\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def normalize_filename(text):\n",
    "    \"\"\"Cleans text to be suitable for a filename.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\-_\\. ]', '', text)\n",
    "    text = re.sub(r'[ ]+', '_', text)\n",
    "    text = text.strip('_')\n",
    "    return text[:100]\n",
    "\n",
    "# --- Main Scraping Logic (mostly same as before, with minor debug changes) ---\n",
    "\n",
    "def scrape_tds_course_content():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping of course content based on the provided screenshot,\n",
    "    ignoring discussion threads.\n",
    "    \"\"\"\n",
    "    print(f\"Starting course content scraping from {BASE_URL}\")\n",
    "    index_page_html = get_page_content(BASE_URL)\n",
    "\n",
    "    if not index_page_html:\n",
    "        print(\"Could not retrieve the main course page. Check the error messages above. Exiting.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(index_page_html, 'html.parser')\n",
    "\n",
    "    # --- 1. Scrape the main overview text ---\n",
    "    main_content_container = soup.find('main') or soup.find('div', class_='main-content') or soup.find('div', id='content') or soup.find('body')\n",
    "    \n",
    "    if main_content_container:\n",
    "        overview_soup = BeautifulSoup(str(main_content_container), 'html.parser')\n",
    "        \n",
    "        # Specific cleaning for overview: remove all links\n",
    "        for a_tag in overview_soup.find_all('a'):\n",
    "            a_tag.decompose()\n",
    "\n",
    "        # Try to remove elements that look like module list items from the overview_soup\n",
    "        # This is the tricky part without direct HTML.\n",
    "        # Let's try to remove elements that contain text matching module patterns.\n",
    "        # This is a heuristic. You might need to inspect the HTML of the module list.\n",
    "        for elem in overview_soup.find_all(lambda tag: tag.name in ['div', 'p', 'li'] and (re.match(r'^\\d+\\.\\s', tag.get_text(strip=True)) or re.match(r'^Project\\s\\d+', tag.get_text(strip=True), re.IGNORECASE))):\n",
    "            elem.decompose() # Remove the whole element if it matches a module pattern\n",
    "\n",
    "        overview_text_content = clean_html_text(overview_soup) \n",
    "        if overview_text_content.strip(): # Only save if there's actual content\n",
    "            save_content(\"course_overview.txt\", overview_text_content)\n",
    "            print(\"Extracted overview text.\")\n",
    "        else:\n",
    "            print(\"Warning: Overview text extraction resulted in blank content. Adjusting selectors needed.\")\n",
    "            save_content(\"course_overview_blank_debug.txt\", \"Blank content after processing.\")\n",
    "    else:\n",
    "        print(\"Warning: Could not find a suitable main content container for overview. Overview text might be incomplete.\")\n",
    "        save_content(\"course_overview_fallback.txt\", clean_html_text(soup)) # Fallback: clean entire page for overview\n",
    "\n",
    "    # --- 2. Identify Module and Project Links ---\n",
    "    module_links_found = []\n",
    "    \n",
    "    all_potential_links_on_page = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_potential_links_on_page:\n",
    "        href = link.get('href')\n",
    "        text = link.get_text().strip()\n",
    "        \n",
    "        if not href or href.startswith(('#', 'javascript:')):\n",
    "            continue\n",
    "        \n",
    "        if \"discussion thread\" in text.lower():\n",
    "            continue\n",
    "            \n",
    "        if not href.startswith(('http://', 'https://')):\n",
    "            abs_url = requests.compat.urljoin(BASE_URL, href)\n",
    "        else:\n",
    "            abs_url = href\n",
    "        \n",
    "        if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE):\n",
    "            if abs_url.startswith(BASE_URL.split('/')[0] + '//' + BASE_URL.split('/')[2]):\n",
    "                module_links_found.append({'url': abs_url, 'text': text})\n",
    "            else:\n",
    "                print(f\"Skipping external link for module: {text} -> {abs_url}\")\n",
    "        \n",
    "    unique_module_links = []\n",
    "    processed_urls_for_modules = set()\n",
    "    for link_info in module_links_found:\n",
    "        if link_info['url'] not in processed_urls_for_modules:\n",
    "            unique_module_links.append(link_info)\n",
    "            processed_urls_for_modules.add(link_info['url'])\n",
    "\n",
    "    print(f\"Found {len(unique_module_links)} unique module/project content links.\")\n",
    "\n",
    "    # --- 3. Visit Each Module Page and Scrape Content ---\n",
    "    for i, link_info in enumerate(unique_module_links):\n",
    "        url = link_info['url']\n",
    "        original_link_text = link_info['text']\n",
    "        filename_prefix = normalize_filename(original_link_text)\n",
    "        \n",
    "        print(f\"\\nProcessing content from module: {original_link_text} ({url})\")\n",
    "\n",
    "        module_page_html = get_page_content(url)\n",
    "        if module_page_html:\n",
    "            # For module pages, don't remove <a> tags generally in clean_html_text,\n",
    "            # as they might be part of the module content (e.g., links to resources)\n",
    "            # Create a new BeautifulSoup object for the module page to clean it.\n",
    "            module_soup = BeautifulSoup(module_page_html, 'html.parser')\n",
    "            \n",
    "            # For cleaning module pages, we might want a slightly different 'clean_html_text' logic\n",
    "            # specifically for this context, or make clean_html_text configurable.\n",
    "            # For now, let's just make sure it's not decomposing <a> tags globally.\n",
    "            # (The clean_html_text function's 'a' tag removal was specifically for the overview,\n",
    "            # so we ensure it's not applied here if it causes issues on module pages.)\n",
    "            \n",
    "            # Re-checking clean_html_text: it *does* decompose 'a'. This might be too aggressive for module content.\n",
    "            # Let's create a *separate* cleaning function for module pages that *doesn't* remove links.\n",
    "            def clean_module_page_text(html_content_for_module):\n",
    "                soup_module = BeautifulSoup(html_content_for_module, 'html.parser')\n",
    "                # Remove script, style, header, footer, nav, aside, form, img, svg - but KEEP 'a'\n",
    "                for element_to_decompose in soup_module(['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'img', 'svg']):\n",
    "                    element_to_decompose.decompose()\n",
    "                \n",
    "                text = soup_module.get_text(separator='\\n')\n",
    "                text = re.sub(r'\\n+', '\\n', text)\n",
    "                text = re.sub(r' +', ' ', text)\n",
    "                text = text.strip()\n",
    "                return text\n",
    "\n",
    "            cleaned_text = clean_module_page_text(module_page_html) # Use specialized cleaner\n",
    "            \n",
    "            if cleaned_text.strip():\n",
    "                filename = f\"{filename_prefix}.txt\"\n",
    "                save_content(filename, cleaned_text)\n",
    "            else:\n",
    "                print(f\"Warning: No significant text extracted from {url} after cleaning.\")\n",
    "                save_content(f\"{filename_prefix}_blank_debug.txt\", \"Blank content after cleaning.\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for module: {original_link_text} ({url})\")\n",
    "\n",
    "    print(\"\\nCourse content scraping finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(COURSE_CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"--- Starting IIT Madras TDS Course Content Scraping (HTML Only) ---\")\n",
    "    print(\"WARNING: This script is a template. You MUST customize `BASE_URL`.\")\n",
    "    print(\"If output is blank, check the 'raw_*.html' files in the debug directory.\")\n",
    "    print(\"Remember to handle authentication if the content is behind a login.\")\n",
    "    print(\"Ensure you have permission to scrape the website.\")\n",
    "    \n",
    "    scrape_tds_course_content()\n",
    "    print(\"--- Scraping script execution complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a294cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting IIT Madras TDS Course Content Scraping (Selenium - Microsoft Edge) ---\n",
      "WARNING: This script now uses Selenium with MSEdgeDriver. Ensure it's installed.\n",
      "Ensure WEBDRIVER_PATH is correct: 'C:\\Users\\sahil\\Desktop\\TA_Project\\msedgedriver.exe'\n",
      "Remember to handle authentication if the content is behind a login.\n",
      "Ensure you have permission to scrape the website.\n",
      "Starting course content scraping from https://tds.s-anand.net/#/2025-01/\n",
      "WebDriver (Microsoft Edge) initialized successfully.\n",
      "Attempting to load page with Selenium: https://tds.s-anand.net/#/2025-01/\n",
      "Error loading page with Selenium https://tds.s-anand.net/#/2025-01/: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff7ce293865+25605]\n",
      "\t(No symbol) [0x0x7ff7ce1e3970]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce516e7a+1962506]\n",
      "\t(No symbol) [0x0x7ff7cdff86c4]\n",
      "\t(No symbol) [0x0x7ff7cdff898b]\n",
      "\t(No symbol) [0x0x7ff7ce039af7]\n",
      "\t(No symbol) [0x0x7ff7ce019e8f]\n",
      "\t(No symbol) [0x0x7ff7cdfeea5d]\n",
      "\t(No symbol) [0x0x7ff7ce0375df]\n",
      "\t(No symbol) [0x0x7ff7ce019bb3]\n",
      "\t(No symbol) [0x0x7ff7cdfedf56]\n",
      "\t(No symbol) [0x0x7ff7cdfed463]\n",
      "\t(No symbol) [0x0x7ff7cdfedd83]\n",
      "\t(No symbol) [0x0x7ff7ce0ee10d]\n",
      "\t(No symbol) [0x0x7ff7ce0fdce8]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce378839+265161]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce380111+296097]\n",
      "\t(No symbol) [0x0x7ff7ce1f2111]\n",
      "\t(No symbol) [0x0x7ff7ce1ea5b4]\n",
      "\t(No symbol) [0x0x7ff7ce1ea703]\n",
      "\t(No symbol) [0x0x7ff7ce1db8d6]\n",
      "\tBaseThreadInitThunk [0x0x7ffecce2e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffecd93c5dc+44]\n",
      "\n",
      "Could not retrieve the main course page with Selenium. Exiting.\n",
      "--- Scraping script execution complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# --- Selenium imports for Edge ---\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service # Change from chrome.service to edge.service\n",
    "from selenium.webdriver.edge.options import Options # Import Options specifically for Edge\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_URL = \"https://tds.s-anand.net/#/2025-01/\" # <--- Your confirmed URL!\n",
    "\n",
    "# Output directory to save scraped course content\n",
    "COURSE_CONTENT_OUTPUT_DIR = \"scraped_tds_course_content\"\n",
    "\n",
    "# Path to your MSEdgeDriver executable\n",
    "# IMPORTANT: Replace with the actual path to your msedgedriver.exe\n",
    "WEBDRIVER_PATH = r\"C:\\Users\\sahil\\Desktop\\TA_Project\\msedgedriver.exe\"\n",
    "# WEBDRIVER_PATH = \"/usr/local/bin/msedgedriver\" # For macOS/Linux (unlikely for Edge, but for completeness)\n",
    "\n",
    "# Time to wait for elements to load (JavaScript rendering)\n",
    "WAIT_TIME_SECONDS = 10 \n",
    "\n",
    "# --- Helper Functions (Updated for Edge) ---\n",
    "\n",
    "driver = None\n",
    "\n",
    "def initialize_driver():\n",
    "    global driver\n",
    "    if driver is None:\n",
    "        try:\n",
    "            # Configure Edge options\n",
    "            edge_options = Options() # Use Options for Edge\n",
    "            # edge_options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "            edge_options.add_argument(\"--disable-gpu\")\n",
    "            edge_options.add_argument(\"--no-sandbox\")\n",
    "            edge_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            edge_options.add_argument(\"window-size=1920x1080\")\n",
    "            \n",
    "            service = Service(WEBDRIVER_PATH) # Use Service for Edge\n",
    "            driver = webdriver.Edge(service=service, options=edge_options) # Use webdriver.Edge\n",
    "            print(\"WebDriver (Microsoft Edge) initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing WebDriver: {e}\")\n",
    "            print(\"Make sure msedgedriver is installed and its path is correct.\")\n",
    "            print(\"Also, ensure msedgedriver version matches your Edge browser version.\")\n",
    "            exit()\n",
    "\n",
    "def get_page_content_selenium(url):\n",
    "    \"\"\"Fetches the content of a given URL using Selenium.\"\"\"\n",
    "    if driver is None:\n",
    "        initialize_driver()\n",
    "        \n",
    "    try:\n",
    "        print(f\"Attempting to load page with Selenium: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # --- IMPORTANT: Wait for content to load ---\n",
    "        WebDriverWait(driver, WAIT_TIME_SECONDS).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h2[contains(text(), 'Tools in Data Science - Jan 2025')]\"))\n",
    "        )\n",
    "        print(\"Page content loaded (H2 header found).\")\n",
    "        \n",
    "        time.sleep(1) # Small additional delay just to be safe after content loads\n",
    "        \n",
    "        return driver.page_source\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page with Selenium {url}: {e}\")\n",
    "        driver.save_screenshot(os.path.join(COURSE_CONTENT_OUTPUT_DIR, \"error_screenshot.png\"))\n",
    "        return None\n",
    "\n",
    "def save_content(filename, content, directory=COURSE_CONTENT_OUTPUT_DIR):\n",
    "    \"\"\"Saves content to a file in the specified directory.\"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving {filepath}: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_html_text(html_element, remove_links=False):\n",
    "    \"\"\"\n",
    "    Removes common HTML tags and cleans up text from a given BeautifulSoup element.\n",
    "    `remove_links` controls whether <a> tags are decomposed.\n",
    "    \"\"\"\n",
    "    temp_soup = BeautifulSoup(str(html_element), 'html.parser')\n",
    "\n",
    "    elements_to_decompose = ['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'img', 'svg']\n",
    "    if remove_links:\n",
    "        elements_to_decompose.append('a')\n",
    "\n",
    "    for element in temp_soup(elements_to_decompose):\n",
    "        element.decompose()\n",
    "\n",
    "    text = temp_soup.get_text(separator='\\n')\n",
    "\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def normalize_filename(text):\n",
    "    \"\"\"Cleans text to be suitable for a filename.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\-_\\. ]', '', text)\n",
    "    text = re.sub(r'[ ]+', '_', text)\n",
    "    text = text.strip('_')\n",
    "    return text[:100]\n",
    "\n",
    "# --- Main Scraping Logic (same as previous Selenium version) ---\n",
    "\n",
    "def scrape_tds_course_content():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the scraping of course content using Selenium.\n",
    "    \"\"\"\n",
    "    print(f\"Starting course content scraping from {BASE_URL}\")\n",
    "    \n",
    "    index_page_html = get_page_content_selenium(BASE_URL)\n",
    "\n",
    "    if not index_page_html:\n",
    "        print(\"Could not retrieve the main course page with Selenium. Exiting.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(index_page_html, 'html.parser')\n",
    "\n",
    "    # --- 1. Scrape the main overview text ---\n",
    "    # Find a main content container based on your HTML structure.\n",
    "    # You might need to inspect the HTML for the actual class/id of this container.\n",
    "    main_content_container = soup.find('main') or soup.find('div', class_='content-wrapper') or soup.find('div', id='root') or soup.find('body') # <-- Adjust this selector\n",
    "    \n",
    "    if main_content_container:\n",
    "        overview_soup = BeautifulSoup(str(main_content_container), 'html.parser')\n",
    "        \n",
    "        for link_elem in overview_soup.find_all('a', href=True):\n",
    "            text = link_elem.get_text(strip=True)\n",
    "            if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE) or \"discussion thread\" in text.lower():\n",
    "                if link_elem.parent and link_elem.parent.name in ['li', 'div', 'p']:\n",
    "                    link_elem.parent.decompose()\n",
    "                else:\n",
    "                    link_elem.decompose()\n",
    "\n",
    "        overview_text_content = clean_html_text(overview_soup, remove_links=True) \n",
    "        if overview_text_content.strip():\n",
    "            save_content(\"course_overview.txt\", overview_text_content)\n",
    "            print(\"Extracted overview text.\")\n",
    "        else:\n",
    "            print(\"Warning: Overview text extraction resulted in blank content after cleaning. Adjusting selectors needed.\")\n",
    "            save_content(\"course_overview_blank_debug.txt\", \"Blank content after processing (overview).\")\n",
    "    else:\n",
    "        print(\"Warning: Could not find a suitable main content container for overview. Fallback to cleaning entire page.\")\n",
    "        save_content(\"course_overview_fallback.txt\", clean_html_text(soup, remove_links=True)) \n",
    "\n",
    "    # --- 2. Identify Module and Project Links ---\n",
    "    module_links_found = []\n",
    "    \n",
    "    potential_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in potential_links:\n",
    "        href = link.get('href')\n",
    "        text = link.get_text().strip()\n",
    "        \n",
    "        if not href or href.startswith(('#', 'javascript:')):\n",
    "            continue\n",
    "        \n",
    "        if \"discussion thread\" in text.lower() or \"search\" in text.lower() or \"login\" in text.lower():\n",
    "            continue\n",
    "            \n",
    "        if not href.startswith(('http://', 'https://')):\n",
    "            abs_url = requests.compat.urljoin(BASE_URL, href)\n",
    "        else:\n",
    "            abs_url = href\n",
    "        \n",
    "        if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE):\n",
    "            if abs_url.startswith(BASE_URL.split('/')[0] + '//' + BASE_URL.split('/')[2]):\n",
    "                module_links_found.append({'url': abs_url, 'text': text})\n",
    "            else:\n",
    "                print(f\"Skipping external link for module: {text} -> {abs_url}\")\n",
    "        \n",
    "    unique_module_links = []\n",
    "    processed_urls_for_modules = set()\n",
    "    for link_info in module_links_found:\n",
    "        if link_info['url'] not in processed_urls_for_modules:\n",
    "            unique_module_links.append(link_info)\n",
    "            processed_urls_for_modules.add(link_info['url'])\n",
    "\n",
    "    print(f\"Found {len(unique_module_links)} unique module/project content links.\")\n",
    "\n",
    "    # --- 3. Visit Each Module Page and Scrape Content ---\n",
    "    for i, link_info in enumerate(unique_module_links):\n",
    "        url = link_info['url']\n",
    "        original_link_text = link_info['text']\n",
    "        filename_prefix = normalize_filename(original_link_text)\n",
    "        \n",
    "        print(f\"\\nProcessing content from module: {original_link_text} ({url})\")\n",
    "\n",
    "        module_page_html = get_page_content_selenium(url)\n",
    "        if module_page_html:\n",
    "            module_soup = BeautifulSoup(module_page_html, 'html.parser')\n",
    "            cleaned_text = clean_html_text(module_soup, remove_links=False) \n",
    "            \n",
    "            if cleaned_text.strip():\n",
    "                filename = f\"{filename_prefix}.txt\"\n",
    "                save_content(filename, cleaned_text)\n",
    "            else:\n",
    "                print(f\"Warning: No significant text extracted from {url} after cleaning. Saved blank debug file.\")\n",
    "                save_content(f\"{filename_prefix}_blank_debug.txt\", \"Blank content after processing (module).\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for module: {original_link_text} ({url})\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"WebDriver closed.\")\n",
    "    print(\"\\nCourse content scraping finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(COURSE_CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"--- Starting IIT Madras TDS Course Content Scraping (Selenium - Microsoft Edge) ---\")\n",
    "    print(\"WARNING: This script now uses Selenium with MSEdgeDriver. Ensure it's installed.\")\n",
    "    print(f\"Ensure WEBDRIVER_PATH is correct: '{WEBDRIVER_PATH}'\")\n",
    "    print(\"Remember to handle authentication if the content is behind a login.\")\n",
    "    print(\"Ensure you have permission to scrape the website.\")\n",
    "    \n",
    "    scrape_tds_course_content()\n",
    "    print(\"--- Scraping script execution complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcacb703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting IIT Madras TDS Course Content Scraping (Selenium - Microsoft Edge) ---\n",
      "WARNING: This script now uses Selenium with MSEdgeDriver. Ensure it's installed.\n",
      "Ensure WEBDRIVER_PATH is correct: 'C:\\Users\\sahil\\Desktop\\TA_Project\\msedgedriver.exe'\n",
      "If browser window does not appear or shows errors, ensure MSEdgeDriver version matches your Edge browser version exactly.\n",
      "Remember to handle authentication if the content is behind a login.\n",
      "Ensure you have permission to scrape the website.\n",
      "Starting course content scraping from https://tds.s-anand.net/#/README?id=tools-in-data-science-may-2025\n",
      "WebDriver (Microsoft Edge) initialized successfully.\n",
      "Attempting to load page with Selenium: https://tds.s-anand.net/#/README?id=tools-in-data-science-may-2025\n",
      "Error loading page with Selenium https://tds.s-anand.net/#/README?id=tools-in-data-science-may-2025: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff7ce293865+25605]\n",
      "\t(No symbol) [0x0x7ff7ce1e3970]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce516e7a+1962506]\n",
      "\t(No symbol) [0x0x7ff7cdff86c4]\n",
      "\t(No symbol) [0x0x7ff7cdff898b]\n",
      "\t(No symbol) [0x0x7ff7ce039af7]\n",
      "\t(No symbol) [0x0x7ff7ce019e8f]\n",
      "\t(No symbol) [0x0x7ff7cdfeea5d]\n",
      "\t(No symbol) [0x0x7ff7ce0375df]\n",
      "\t(No symbol) [0x0x7ff7ce019bb3]\n",
      "\t(No symbol) [0x0x7ff7cdfedf56]\n",
      "\t(No symbol) [0x0x7ff7cdfed463]\n",
      "\t(No symbol) [0x0x7ff7cdfedd83]\n",
      "\t(No symbol) [0x0x7ff7ce0ee10d]\n",
      "\t(No symbol) [0x0x7ff7ce0fdce8]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce378839+265161]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce380111+296097]\n",
      "\t(No symbol) [0x0x7ff7ce1f2111]\n",
      "\t(No symbol) [0x0x7ff7ce1ea5b4]\n",
      "\t(No symbol) [0x0x7ff7ce1ea703]\n",
      "\t(No symbol) [0x0x7ff7ce1db8d6]\n",
      "\tBaseThreadInitThunk [0x0x7ffecce2e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffecd93c5dc+44]\n",
      "\n",
      "Saved error screenshot to scraped_tds_course_content\\error_screenshot_after_load_attempt.png\n",
      "Could not retrieve the main course page with Selenium. Exiting.\n",
      "--- Scraping script execution complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_URL = \"https://tds.s-anand.net/#/README?id=tools-in-data-science-may-2025\" # Your confirmed URL!\n",
    "COURSE_CONTENT_OUTPUT_DIR = \"scraped_tds_course_content\"\n",
    "WEBDRIVER_PATH = r\"C:\\Users\\sahil\\Desktop\\TA_Project\\msedgedriver.exe\" # Using raw string for path\n",
    "WAIT_TIME_SECONDS = 15 # Increased wait time\n",
    "\n",
    "driver = None\n",
    "\n",
    "def initialize_driver():\n",
    "    global driver\n",
    "    if driver is None:\n",
    "        try:\n",
    "            edge_options = Options()\n",
    "            # UNCOMMENT THE LINE BELOW TO SEE THE BROWSER WINDOW FOR DEBUGGING\n",
    "            # edge_options.add_argument(\"--headless\")\n",
    "            edge_options.add_argument(\"--disable-gpu\")\n",
    "            edge_options.add_argument(\"--no-sandbox\")\n",
    "            edge_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            edge_options.add_argument(\"window-size=1920x1080\")\n",
    "            \n",
    "            service = Service(WEBDRIVER_PATH)\n",
    "            driver = webdriver.Edge(service=service, options=edge_options)\n",
    "            print(\"WebDriver (Microsoft Edge) initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing WebDriver: {e}\")\n",
    "            print(f\"Make sure msedgedriver is installed, its path '{WEBDRIVER_PATH}' is correct, and its version matches your Edge browser version.\")\n",
    "            exit()\n",
    "\n",
    "def get_page_content_selenium(url):\n",
    "    \"\"\"Fetches the content of a given URL using Selenium.\"\"\"\n",
    "    if driver is None:\n",
    "        initialize_driver()\n",
    "        \n",
    "    try:\n",
    "        print(f\"Attempting to load page with Selenium: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # --- IMPORTANT: Wait for a reliable element to load ---\n",
    "        # The H2 might not be immediately visible. Let's try to wait for a more generic element\n",
    "        # that indicates the *page has fully loaded its basic structure* from JS.\n",
    "        # Given the screenshot, the left sidebar element with \"Tools in Data Science\"\n",
    "        # or the \"Tools in Data Science - Jan 2025\" header itself should be reliable targets.\n",
    "        \n",
    "        # Waiting for the main H2 title to be present and visible\n",
    "        WebDriverWait(driver, WAIT_TIME_SECONDS).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, \"//h2[contains(text(), 'Tools in Data Science - Jan 2025')]\"))\n",
    "        )\n",
    "        print(\"Page content loaded (H2 header found and visible).\")\n",
    "        \n",
    "        # You might also add a small fixed delay after the elements are expected to load\n",
    "        # to ensure all dynamic content has settled.\n",
    "        time.sleep(2) \n",
    "        \n",
    "        return driver.page_source\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading page with Selenium {url}: {e}\")\n",
    "        # Save a screenshot *if* an error occurs, to see the state of the browser\n",
    "        try:\n",
    "            driver.save_screenshot(os.path.join(COURSE_CONTENT_OUTPUT_DIR, \"error_screenshot_after_load_attempt.png\"))\n",
    "            print(f\"Saved error screenshot to {os.path.join(COURSE_CONTENT_OUTPUT_DIR, 'error_screenshot_after_load_attempt.png')}\")\n",
    "        except Exception as ss_e:\n",
    "            print(f\"Could not save screenshot: {ss_e}\")\n",
    "        return None\n",
    "\n",
    "def save_content(filename, content, directory=COURSE_CONTENT_OUTPUT_DIR):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving {filepath}: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_html_text(html_element, remove_links=False):\n",
    "    temp_soup = BeautifulSoup(str(html_element), 'html.parser')\n",
    "    elements_to_decompose = ['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'img', 'svg']\n",
    "    if remove_links:\n",
    "        elements_to_decompose.append('a')\n",
    "    for element in temp_soup(elements_to_decompose):\n",
    "        element.decompose()\n",
    "    text = temp_soup.get_text(separator='\\n')\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def normalize_filename(text):\n",
    "    text = re.sub(r'[^\\w\\-_\\. ]', '', text)\n",
    "    text = re.sub(r'[ ]+', '_', text)\n",
    "    text = text.strip('_')\n",
    "    return text[:100]\n",
    "\n",
    "# --- Main Scraping Logic ---\n",
    "\n",
    "def scrape_tds_course_content():\n",
    "    print(f\"Starting course content scraping from {BASE_URL}\")\n",
    "    \n",
    "    index_page_html = get_page_content_selenium(BASE_URL)\n",
    "\n",
    "    if not index_page_html:\n",
    "        print(\"Could not retrieve the main course page with Selenium. Exiting.\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(index_page_html, 'html.parser')\n",
    "\n",
    "    # --- 1. Scrape the main overview text ---\n",
    "    # Attempt to find a main content container based on your HTML structure.\n",
    "    # From the screenshot, it looks like the content is within a main area.\n",
    "    # Common guesses for SPAs: <div id=\"app\">, <div id=\"root\">, <main>, <div class=\"main-content\">\n",
    "    # Try using the `div` with class `sc-iomgmy` or similar if inspecting the live page.\n",
    "    main_content_container = soup.find('main') or soup.find('div', class_='main-content') or soup.find('div', id='root') or soup.find('body') \n",
    "    \n",
    "    if main_content_container:\n",
    "        overview_soup = BeautifulSoup(str(main_content_container), 'html.parser')\n",
    "        \n",
    "        # Remove elements that are part of the module list structure from the overview_soup\n",
    "        # This is a heuristic. You might need to refine this by looking for specific module list containers.\n",
    "        for link_elem in overview_soup.find_all('a', href=True):\n",
    "            text = link_elem.get_text(strip=True)\n",
    "            if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE) or \"discussion thread\" in text.lower():\n",
    "                # Try to remove the entire block containing the link\n",
    "                # Look for a common parent that groups the number, title, and discussion link\n",
    "                parent_to_remove = link_elem.find_parent(['li', 'div', 'p']) # Common parent types for list items/blocks\n",
    "                if parent_to_remove:\n",
    "                    parent_to_remove.decompose()\n",
    "                else:\n",
    "                    link_elem.decompose() # Fallback if no suitable parent found\n",
    "        \n",
    "        overview_text_content = clean_html_text(overview_soup, remove_links=True) \n",
    "        if overview_text_content.strip():\n",
    "            save_content(\"course_overview.txt\", overview_text_content)\n",
    "            print(\"Extracted overview text.\")\n",
    "        else:\n",
    "            print(\"Warning: Overview text extraction resulted in blank content after cleaning. Adjusting selectors needed.\")\n",
    "            save_content(\"course_overview_blank_debug.txt\", \"Blank content after processing (overview).\")\n",
    "    else:\n",
    "        print(\"Warning: Could not find a suitable main content container for overview. Fallback to cleaning entire page.\")\n",
    "        save_content(\"course_overview_fallback.txt\", clean_html_text(soup, remove_links=True)) \n",
    "\n",
    "    # --- 2. Identify Module and Project Links ---\n",
    "    module_links_found = []\n",
    "    \n",
    "    # Target elements that contain both the module number/title and the link itself.\n",
    "    # Based on your screenshots, these look like <p> tags or similar blocks.\n",
    "    \n",
    "    # Try to find all <a> tags that match the pattern, anywhere on the page\n",
    "    # You might also look for parent elements of these links if they are consistently structured.\n",
    "    # For example, if they are all inside a <div class=\"course-modules\">\n",
    "    # content_area = soup.find('div', class_='course-modules')\n",
    "    # if content_area:\n",
    "    #     potential_links = content_area.find_all('a', href=True)\n",
    "    # else:\n",
    "    #     potential_links = soup.find_all('a', href=True) # Fallback\n",
    "    \n",
    "    potential_links = soup.find_all('a', href=True) # Start broad if not sure about parent container\n",
    "    \n",
    "    for link in potential_links:\n",
    "        href = link.get('href')\n",
    "        text = link.get_text().strip()\n",
    "        \n",
    "        if not href or href.startswith(('#', 'javascript:')):\n",
    "            continue\n",
    "        \n",
    "        if \"discussion thread\" in text.lower() or \"search\" in text.lower() or \"login\" in text.lower():\n",
    "            continue\n",
    "            \n",
    "        if not href.startswith(('http://', 'https://')):\n",
    "            abs_url = requests.compat.urljoin(BASE_URL, href)\n",
    "        else:\n",
    "            abs_url = href\n",
    "        \n",
    "        if re.match(r'^\\d+\\.\\s', text) or re.match(r'^Project\\s\\d+', text, re.IGNORECASE):\n",
    "            if abs_url.startswith(BASE_URL.split('/')[0] + '//' + BASE_URL.split('/')[2]):\n",
    "                module_links_found.append({'url': abs_url, 'text': text})\n",
    "            else:\n",
    "                print(f\"Skipping external link for module: {text} -> {abs_url}\")\n",
    "        \n",
    "    unique_module_links = []\n",
    "    processed_urls_for_modules = set()\n",
    "    for link_info in module_links_found:\n",
    "        if link_info['url'] not in processed_urls_for_modules:\n",
    "            unique_module_links.append(link_info)\n",
    "            processed_urls_for_modules.add(link_info['url'])\n",
    "\n",
    "    print(f\"Found {len(unique_module_links)} unique module/project content links.\")\n",
    "\n",
    "    # --- 3. Visit Each Module Page and Scrape Content ---\n",
    "    for i, link_info in enumerate(unique_module_links):\n",
    "        url = link_info['url']\n",
    "        original_link_text = link_info['text']\n",
    "        filename_prefix = normalize_filename(original_link_text)\n",
    "        \n",
    "        print(f\"\\nProcessing content from module: {original_link_text} ({url})\")\n",
    "\n",
    "        module_page_html = get_page_content_selenium(url)\n",
    "        if module_page_html:\n",
    "            module_soup = BeautifulSoup(module_page_html, 'html.parser')\n",
    "            # For module pages, we generally *do not* want to remove links as they might be relevant content.\n",
    "            cleaned_text = clean_html_text(module_soup, remove_links=False) \n",
    "            \n",
    "            if cleaned_text.strip():\n",
    "                filename = f\"{filename_prefix}.txt\"\n",
    "                save_content(filename, cleaned_text)\n",
    "            else:\n",
    "                print(f\"Warning: No significant text extracted from {url} after cleaning. Saved blank debug file.\")\n",
    "                save_content(f\"{filename_prefix}_blank_debug.txt\", \"Blank content after processing (module).\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content for module: {original_link_text} ({url})\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"WebDriver closed.\")\n",
    "    print(\"\\nCourse content scraping finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(COURSE_CONTENT_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"--- Starting IIT Madras TDS Course Content Scraping (Selenium - Microsoft Edge) ---\")\n",
    "    print(\"WARNING: This script now uses Selenium with MSEdgeDriver. Ensure it's installed.\")\n",
    "    print(f\"Ensure WEBDRIVER_PATH is correct: '{WEBDRIVER_PATH}'\")\n",
    "    print(\"If browser window does not appear or shows errors, ensure MSEdgeDriver version matches your Edge browser version exactly.\")\n",
    "    print(\"Remember to handle authentication if the content is behind a login.\")\n",
    "    print(\"Ensure you have permission to scrape the website.\")\n",
    "    \n",
    "    scrape_tds_course_content()\n",
    "    print(\"--- Scraping script execution complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bdea41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7456a5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load page: https://tds.s-anand.net/#/2025-01/\n",
      "Page loaded successfully (or so it seems).\n",
      "Waiting for the main heading to be present...\n",
      "Error loading page or finding elements: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff7ce293865+25605]\n",
      "\t(No symbol) [0x0x7ff7ce1e3970]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce516e7a+1962506]\n",
      "\t(No symbol) [0x0x7ff7cdff86c4]\n",
      "\t(No symbol) [0x0x7ff7cdff898b]\n",
      "\t(No symbol) [0x0x7ff7ce039af7]\n",
      "\t(No symbol) [0x0x7ff7ce019e8f]\n",
      "\t(No symbol) [0x0x7ff7cdfeea5d]\n",
      "\t(No symbol) [0x0x7ff7ce0375df]\n",
      "\t(No symbol) [0x0x7ff7ce019bb3]\n",
      "\t(No symbol) [0x0x7ff7cdfedf56]\n",
      "\t(No symbol) [0x0x7ff7cdfed463]\n",
      "\t(No symbol) [0x0x7ff7cdfedd83]\n",
      "\t(No symbol) [0x0x7ff7ce0ee10d]\n",
      "\t(No symbol) [0x0x7ff7ce0fdce8]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce378839+265161]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce380111+296097]\n",
      "\t(No symbol) [0x0x7ff7ce1f2111]\n",
      "\t(No symbol) [0x0x7ff7ce1ea5b4]\n",
      "\t(No symbol) [0x0x7ff7ce1ea703]\n",
      "\t(No symbol) [0x0x7ff7ce1db8d6]\n",
      "\tBaseThreadInitThunk [0x0x7ffecce2e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffecd93c5dc+44]\n",
      "\n",
      "Closing browser...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Ensure this path is correct for your msedgedriver.exe\n",
    "WEBDRIVER_PATH = r'C:\\Users\\sahil\\Desktop\\TA_Project\\msedgedriver.exe'\n",
    "\n",
    "service = Service(WEBDRIVER_PATH)\n",
    "driver = webdriver.Edge(service=service)\n",
    "\n",
    "# Set an implicit wait time (e.g., 10 seconds)\n",
    "driver.implicitly_wait(10) # Wait up to 10 seconds for elements to appear\n",
    "\n",
    "try:\n",
    "    url = \"https://tds.s-anand.net/#/2025-01/\"\n",
    "    print(f\"Attempting to load page: {url}\")\n",
    "    driver.get(url)\n",
    "    print(\"Page loaded successfully (or so it seems).\")\n",
    "\n",
    "    # Add a short sleep just to visually confirm the page is open\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Maximize the window (can sometimes help with dynamic content rendering)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # --- IMPORTANT: Add an Explicit Wait for a specific element ---\n",
    "    # Look at your screenshot. What's a unique element that appears after the page is fully loaded?\n",
    "    # For example, the \"Tools in Data Science - Jan 2025\" heading.\n",
    "    # Let's try to wait for that h1 element.\n",
    "\n",
    "    print(\"Waiting for the main heading to be present...\")\n",
    "    main_heading_xpath = \"//h1[contains(text(), 'Tools in Data Science - Jan 2025')]\"\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, main_heading_xpath))\n",
    "    )\n",
    "    print(\"Main heading found! Page content seems to be fully loaded.\")\n",
    "\n",
    "    # Now, you can try to interact with other elements, e.g., click a link\n",
    "    # Example: Try to click the \"Development Tools\" link in the left sidebar\n",
    "    # Inspect the element on the actual page to get its correct XPath or CSS selector\n",
    "    try:\n",
    "        dev_tools_link_xpath = \"//a[contains(@href, '#/2025-01/development-tools')]\"\n",
    "        dev_tools_element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, dev_tools_link_xpath))\n",
    "        )\n",
    "        print(\"Clicking 'Development Tools' link...\")\n",
    "        dev_tools_element.click()\n",
    "        print(\"Clicked 'Development Tools'. Waiting for content to change...\")\n",
    "        time.sleep(5) # Give it time to load the new content\n",
    "\n",
    "        # You might want to assert that the URL changed or new content is present\n",
    "        print(f\"Current URL after click: {driver.current_url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not click 'Development Tools' link: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading page or finding elements: {e}\")\n",
    "    # Save screenshot if an error occurs during the main process\n",
    "    driver.save_screenshot(\"error_after_waits.png\")\n",
    "\n",
    "finally:\n",
    "    print(\"Closing browser...\")\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5115758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting IIT Madras TDS Course Content Scraping (Selenium - Microsoft Edge) ---\n",
      "WARNING: This script now uses Selenium with MSEdgeDriver. Ensure it's installed.\n",
      "Ensure WEBDRIVER_PATH is correct: 'C:\\Users\\sahil\\Desktop\\TA_Project\\msedgedriver.exe'\n",
      "If browser window does not appear or shows errors, ensure MSEdgeDriver version matches your Edge browser version exactly.\n",
      "Remember to handle authentication if the content is behind a login.\n",
      "Ensure you have permission to scrape the website.\n",
      "WebDriver (Microsoft Edge) initialized successfully.\n",
      "Attempting to load page: https://tds.s-anand.net/#/2025-01/\n",
      "Page loaded successfully (or so it seems).\n",
      "Waiting for the main heading to be present...\n",
      "Error loading page or finding elements: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff7ce293865+25605]\n",
      "\t(No symbol) [0x0x7ff7ce1e3970]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce516e7a+1962506]\n",
      "\t(No symbol) [0x0x7ff7cdff86c4]\n",
      "\t(No symbol) [0x0x7ff7cdff898b]\n",
      "\t(No symbol) [0x0x7ff7ce039af7]\n",
      "\t(No symbol) [0x0x7ff7ce019e8f]\n",
      "\t(No symbol) [0x0x7ff7cdfeea5d]\n",
      "\t(No symbol) [0x0x7ff7ce0375df]\n",
      "\t(No symbol) [0x0x7ff7ce019bb3]\n",
      "\t(No symbol) [0x0x7ff7cdfedf56]\n",
      "\t(No symbol) [0x0x7ff7cdfed463]\n",
      "\t(No symbol) [0x0x7ff7cdfedd83]\n",
      "\t(No symbol) [0x0x7ff7ce0ee10d]\n",
      "\t(No symbol) [0x0x7ff7ce0fdce8]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce378839+265161]\n",
      "\tMicrosoft::Applications::Events::EventProperty::to_string [0x0x7ff7ce380111+296097]\n",
      "\t(No symbol) [0x0x7ff7ce1f2111]\n",
      "\t(No symbol) [0x0x7ff7ce1ea5b4]\n",
      "\t(No symbol) [0x0x7ff7ce1ea703]\n",
      "\t(No symbol) [0x0x7ff7ce1db8d6]\n",
      "\tBaseThreadInitThunk [0x0x7ffecce2e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffecd93c5dc+44]\n",
      "\n",
      "Saved error screenshot to scraped_tds_course_content\\error_screenshot_after_waits.png\n",
      "Closing browser...\n",
      "--- Scraping script execution complete ---\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "\n",
    "WEBDRIVER_PATH = r'C:\\Users\\sahil\\Desktop\\TA_Project\\msedgedriver.exe' # Use raw string for path\n",
    "OUTPUT_DIR = 'scraped_tds_course_content'\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(\"--- Starting IIT Madras TDS Course Content Scraping (Selenium - Microsoft Edge) ---\")\n",
    "print(\"WARNING: This script now uses Selenium with MSEdgeDriver. Ensure it's installed.\")\n",
    "print(f\"Ensure WEBDRIVER_PATH is correct: '{WEBDRIVER_PATH}'\")\n",
    "print(\"If browser window does not appear or shows errors, ensure MSEdgeDriver version matches your Edge browser version exactly.\")\n",
    "print(\"Remember to handle authentication if the content is behind a login.\")\n",
    "print(\"Ensure you have permission to scrape the website.\")\n",
    "\n",
    "options = Options()\n",
    "options.use_chromium = True\n",
    "# Crucial: Set page load strategy to 'eager'\n",
    "options.page_load_strategy = 'eager'\n",
    "# options.add_argument(\"--headless\") # Uncomment for headless mode after testing\n",
    "\n",
    "driver = None\n",
    "try:\n",
    "    service = Service(WEBDRIVER_PATH)\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    print(\"WebDriver (Microsoft Edge) initialized successfully.\")\n",
    "\n",
    "    url = \"https://tds.s-anand.net/#/2025-01/\"\n",
    "    print(f\"Attempting to load page: {url}\")\n",
    "    driver.get(url)\n",
    "    print(\"Page loaded successfully (or so it seems).\")\n",
    "\n",
    "    # Give it a small explicit pause before trying to wait for elements\n",
    "    time.sleep(2) # Added a small delay to let JavaScript execute\n",
    "\n",
    "    print(\"Waiting for the main heading to be present...\")\n",
    "    # Use a more robust selector for the main heading.\n",
    "    # Based on the screenshot, it looks like an h1 or h2.\n",
    "    # You might need to inspect the element in your browser's dev tools (F12)\n",
    "    # to get the exact tag name, class, or id.\n",
    "\n",
    "    # Example using CSS_SELECTOR (adjust as needed after inspecting the element)\n",
    "    main_heading_selector = \"h1.q-page__title\" # This is a common pattern for Quasar framework titles\n",
    "    # If it's just an h1 or h2 without a specific class, use:\n",
    "    # main_heading_selector = \"h1\" \n",
    "    # Or if it has an ID:\n",
    "    # main_heading_selector = \"#your_heading_id\"\n",
    "\n",
    "    WebDriverWait(driver, 30).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, main_heading_selector))\n",
    "    )\n",
    "    print(\"Main heading found!\")\n",
    "\n",
    "    # Now that the main heading is confirmed, you can proceed with other interactions\n",
    "    # For example, finding all links in the main content area\n",
    "    # This part of your script would go here:\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Find all link elements. You'll need to refine this based on the actual HTML structure\n",
    "    # links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    # for link in links:\n",
    "    #     href = link.get_attribute('href')\n",
    "    #     text = link.text\n",
    "    #     if href and \"Discussion Thread\" not in text: # Filter out discussion threads if needed\n",
    "    #         print(f\"Found link: {text} -> {href}\")\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading page or finding elements: {e}\")\n",
    "    error_screenshot_path = os.path.join(OUTPUT_DIR, \"error_screenshot_after_waits.png\")\n",
    "    driver.save_screenshot(error_screenshot_path)\n",
    "    print(f\"Saved error screenshot to {error_screenshot_path}\")\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"Closing browser...\")\n",
    "    print(\"--- Scraping script execution complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d5f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
